{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5PMR-533jsJ"
   },
   "source": [
    "# **Generate data and install Mosek**\n",
    "\n",
    "Please install Mosek according to the instructions on [https://docs.mosek.com/latest/install/installation.html](https://docs.mosek.com/latest/install/installation.html).\n",
    "\n",
    "Before running this cell, run the following in bash - \n",
    "\n",
    "`export PATH=/root/mosek/9.3/tools/platform/linux64x86/bin:$PATH`.\n",
    "\n",
    "Please download MOSEK from [here](https://drive.google.com/drive/folders/1ZIKizbByyQIZmWCPTgFvnnQ6bVzUcRvF?usp=sharing) and add all contents to the current working directory. \n",
    "\n",
    "Please replace *mosek.lic* with your own license file obtained from [here](https://www.mosek.com/products/academic-licenses/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VtcTLLJIHHF"
   },
   "outputs": [],
   "source": [
    "! bash install-mosek.sh\n",
    "! bash create-leopard-environment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtP3I8wtFuj9"
   },
   "source": [
    "# **LEOPARD**\n",
    "\n",
    "Setup the environment required for the classfications which are defined by Bansal et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiRKVWWY6WEy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATA_PATH = \"/content/leopard/data/json\"\n",
    "\n",
    "def get_categories():\n",
    "    categories = []\n",
    "    categories.append(\"restaurant\")\n",
    "    categories.append(\"conll\")\n",
    "    return categories\n",
    "\n",
    "def get_labelled_training_sentences(category, shot, episode):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    entities = []\n",
    "    label_keys = {}\n",
    "    label_index = 0\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_\" + str(episode) + \"_\" + str(shot) + \".json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                sentence_1 = data[index]['sentence1']\n",
    "                entity = data[index]['sentence2']\n",
    "                label = data[index]['label']\n",
    "                sentences.append(sentence_1)\n",
    "                entities.append(entity)\n",
    "                # convert categorical labels to numeric values\n",
    "                if label not in label_keys:\n",
    "                    label_keys[label] = label_index\n",
    "                    label_index += 1\n",
    "                labels.append(label_keys[label])\n",
    "    return sentences, entities, labels, label_keys\n",
    "\n",
    "def get_labelled_test_sentences(category):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    entities = []\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_eval.json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                sentence_1 = data[index]['sentence1']\n",
    "                entity = data[index]['sentence2']\n",
    "                label = data[index]['label']\n",
    "                sentences.append(sentence_1)\n",
    "                labels.append(label)\n",
    "                entities.append(entity)\n",
    "    \n",
    "    return sentences, entities, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIQZ72A0I82v"
   },
   "source": [
    "# **Less Than One Shot Classification (Sucholutsky et al, 2021)**\n",
    "\n",
    "Original source can be found [here](https://github.com/ilia10000/LO-Shot/blob/master/Paper3/Soft%20Label%20Optimization.ipynb). Run the script to install the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWa13LwHJYHU"
   },
   "outputs": [],
   "source": [
    "! bash create-lo-shot-environment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69r4I4qSLMk8"
   },
   "source": [
    "Load the required scripts and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3TnTswoyBxm"
   },
   "outputs": [],
   "source": [
    "% run lo_shot_definitions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0pBubF0OZfm"
   },
   "source": [
    "# **Extracting the data and creating the training set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3M3iTIjqemj"
   },
   "source": [
    "Define the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQOMfYTEUXL6"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import traceback\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG_TLr5xWQpc"
   },
   "source": [
    "Generate the sentence encoding for each sentence. Store this along with the labels; use it to generate lines and prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smNjhHgfWzg-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_labelled_training_data(category, shot, episode):\n",
    "    sentences, entities, training_labels, label_keys = get_labelled_training_sentences(category, shot, episode)\n",
    "\n",
    "    training_encodings = []\n",
    "    for sentence, entity, label in zip(sentences, entities, training_labels):\n",
    "        inputs = tokenizer(entity, return_tensors=\"pt\")\n",
    "        required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        inputs_indices_sentence = []\n",
    "        for i in range(len(inputs['input_ids'].tolist()[0])):\n",
    "            if inputs['input_ids'].tolist()[0][i] in required_bert_tokens:\n",
    "                required_bert_tokens.remove(inputs['input_ids'].tolist()[0][i])\n",
    "                inputs_indices_sentence.append(i)\n",
    "        encodings = []\n",
    "        for inputs_index_sentence in inputs_indices_sentence:\n",
    "            encodings.append(outputs.last_hidden_state[:,inputs_index_sentence,:].detach().numpy())\n",
    "        encoding = np.stack(encodings).mean(axis=0).reshape(-1)\n",
    "        training_encodings.append(encoding)\n",
    "\n",
    "    return training_encodings, training_labels, label_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL43mUWtfHcE"
   },
   "source": [
    "Get centroids for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckbTb8m5fJV7"
   },
   "outputs": [],
   "source": [
    "def get_labelled_centroids(training_encodings, training_labels):\n",
    "    centroids = []\n",
    "    centroid_labels = []\n",
    "    for label in set(training_labels):\n",
    "        centroids_per_label = []\n",
    "        for i in range(len(training_labels)):\n",
    "            if training_labels[i] == label:\n",
    "                centroids_per_label.append(training_encodings[i])\n",
    "        centroid = np.mean(np.array(centroids_per_label), axis=0)\n",
    "        centroids.append(centroid)\n",
    "        centroid_labels.append(label)\n",
    "    \n",
    "    return centroids, centroid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N54LggdSZU2l"
   },
   "source": [
    "Create the test dataset for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GP50vfRaZUN-"
   },
   "outputs": [],
   "source": [
    "def get_labelled_test_data(category):\n",
    "    sentences, entities, test_labels = get_labelled_test_sentences(category)\n",
    "\n",
    "    test_encodings = []\n",
    "    for sentence, entity, label in zip(sentences, entities, test_labels):\n",
    "        inputs = tokenizer(entity, return_tensors=\"pt\")\n",
    "        required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        inputs_indices_sentence = []\n",
    "        for i in range(len(inputs['input_ids'].tolist()[0])):\n",
    "            if inputs['input_ids'].tolist()[0][i] in required_bert_tokens:\n",
    "                required_bert_tokens.remove(inputs['input_ids'].tolist()[0][i])\n",
    "                inputs_indices_sentence.append(i)\n",
    "        encodings = []\n",
    "        for inputs_index_sentence in inputs_indices_sentence:\n",
    "            encodings.append(outputs.last_hidden_state[:,inputs_index_sentence,:].detach().numpy())\n",
    "        encoding = np.stack(encodings).mean(axis=0).reshape(-1)\n",
    "        test_encodings.append(encoding)\n",
    "\n",
    "    return test_encodings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVzjqFeiET3p"
   },
   "source": [
    "# **Classification using different flavours of KNN**\n",
    "Perform classification on the data using vanilla KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hFTe__1EWit"
   },
   "outputs": [],
   "source": [
    "def classify_with_vanilla_KNN(test_classifications, labeled_centroids, labeled_test_data):\n",
    "  \n",
    "  classifier_KNN = SoftKNN(k=1)\n",
    "  knn_labels = np.zeros((test_classifications, test_classifications))\n",
    "  np.fill_diagonal(knn_labels, 1)\n",
    "  classifier_KNN.fit(labeled_centroids[0], knn_labels)\n",
    "  \n",
    "  vanilla_knn_preds = classifier_KNN.predict(labeled_test_data[0])\n",
    "  vanilla_knn_correct = np.sum(vanilla_knn_preds==labeled_test_data[1])\n",
    "  \n",
    "  return vanilla_knn_correct, len(vanilla_knn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFTreANOHINg"
   },
   "source": [
    "Perform the classification using soft label KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J47648URHLSJ"
   },
   "outputs": [],
   "source": [
    "def get_soft_label_prototypes(lines, labeled_centroids):\n",
    "\n",
    "  classifiers = []\n",
    "  for line in lines:\n",
    "    try:\n",
    "      distX, distY = get_line_prototypes(line, labeled_centroids[0])\n",
    "      classifier = SoftKNN(k=1)\n",
    "      classifier.fit(distX, distY)\n",
    "      classifiers.append(classifier)\n",
    "    except:\n",
    "      classifiers.append(None)\n",
    "\n",
    "  return classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtVVUrzx-aDw"
   },
   "source": [
    "# **Classify and measure metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Kwq29eFvFj"
   },
   "source": [
    "Classify the test data using centroids from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeHDtBDbMb8F"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert(cp.MOSEK in cp.installed_solvers())\n",
    "\n",
    "for category in get_categories():\n",
    "    test_encodings, categorical_test_labels = get_labelled_test_data(category)\n",
    "    for episode in range(10):\n",
    "        for shot in [4,8,16]:\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "\n",
    "            training_encodings, training_labels, label_keys = get_labelled_training_data(category, shot, episode)            \n",
    "            if not training_encodings:\n",
    "                continue\n",
    "            # convert categorical attributes to numeric indices\n",
    "            test_labels = [label_keys[label] for label in categorical_test_labels]        \n",
    "            centroids, centroid_labels = get_labelled_centroids(training_encodings, training_labels)\n",
    "\n",
    "            labeled_training_data_np = np.stack(training_encodings, axis=0)\n",
    "            labeled_test_data_np = np.stack(test_encodings, axis=0)\n",
    "            labeled_centroids_np = np.stack(centroids, axis=0)\n",
    "\n",
    "            labeled_test_data = [labeled_test_data_np[i] for i in range(labeled_test_data_np.shape[0])], [label for label in test_labels]\n",
    "            labeled_centroids = [labeled_centroids_np[i] for i in range(labeled_centroids_np.shape[0])], [label for label in centroid_labels]\n",
    "\n",
    "            test_classifications = max(max(labeled_centroids[1]) for labeled_centroid in labeled_centroids) + 1\n",
    "            required_lines = test_classifications - 1\n",
    "            dimensions = labeled_training_data_np.shape[1]\n",
    "\n",
    "            lines = [line_order_no_endpoints(centroids=labeled_centroids_np, active_classes=np.array(line)) for line in find_lines_R_multiD(dat=labeled_training_data_np, labels=training_labels , dims=dimensions, centroids=labeled_centroids_np, k=required_lines)]\n",
    "            classifiers = get_soft_label_prototypes(lines, labeled_centroids)\n",
    "\n",
    "            for i in range(test_classifications):\n",
    "                points = int(np.sum([True for class_val in labeled_test_data[1] if class_val == i]))\n",
    "                points_required = [labeled_test_data[0][x] for x in range(len(labeled_test_data[1])) if labeled_test_data[1][x] == i]\n",
    "                assignments = []\n",
    "                for point in points_required:\n",
    "                    dists = [dist_to_line_multiD(point, labeled_centroids[0][line[0]], labeled_centroids[0][line[-1]]) for line in lines]\n",
    "                    nearest = np.argmin(dists)\n",
    "                    assignments.append(nearest)\n",
    "                for j in range(len(points_required)):\n",
    "                    if classifiers[assignments[j]] is not None:\n",
    "                        classifier = classifiers[assignments[j]]\n",
    "                        prediction = classifier.predict(points_required[j])\n",
    "                        predictions.append(prediction)\n",
    "                        true_labels.append(i)\n",
    "\n",
    "            print(\"For category\", category, \"and shot =\", str(shot) + \"...\")\n",
    "            print(\"Lines used are\", len(lines))\n",
    "            print(\"Number of classifications are\", test_classifications)\n",
    "            print(\"Macro f1 score is\", f1_score(true_labels, predictions, average='macro'))\n",
    "            print(\"Accuracy is\", accuracy_score(true_labels, predictions))\n",
    "            print(\"Correctly classified points are\", np.sum(np.array(true_labels) == np.array(predictions)), \"/\", len(true_labels), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "leopard-entity-typing-bert.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

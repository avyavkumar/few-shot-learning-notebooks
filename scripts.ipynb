{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create-wsd-environment.sh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the package and run the patch for backward compatibility\n",
    "! rm -rf MetaWSD/\n",
    "! git clone https://github.com/avyavkumar/MetaWSD.git\n",
    "! sed -i -e 's/diffopt.step(loss)/diffopt.step(loss, retain_graph=True)/g' MetaWSD/models/seq_meta.py\n",
    "\n",
    "# install dependencies\n",
    "! pip install -r MetaWSD/requirements.txt\n",
    "! pip install transformers==4.15.0\n",
    "! git clone https://github.com/Nithin-Holla/higher.git\n",
    "! cd higher && python setup.py install\n",
    "! yes | pip uninstall overrides && pip install overrides==3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setup-data.sh**\n",
    "\n",
    "*use only for word-sense disambiguation tasks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data\n",
    "! rm -rf data/\n",
    "! mkdir data\n",
    "! cd data \n",
    "! git clone https://github.com/google-research-datasets/word_sense_disambigation_corpora.git\n",
    "\n",
    "# build the dataset\n",
    "! cd ../\n",
    "! python MetaWSD/scripts/wsd_gen_sense_inventory.py\n",
    "! python MetaWSD/scripts/generate_wsd_data.py --n_support_examples 16 --n_query_examples 16 --n_train_episodes 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_encodings.py**\n",
    "\n",
    "*only use for word-sense disambiguation tasks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "UNDEF = -10000\n",
    "\n",
    "def generate(model, tokenizer, SHOT):\n",
    "  TRAINING_DATA_PATH = \"data/semcor_meta/meta_train_\" + str(SHOT) + \"-\" + str(SHOT)\n",
    "  TEST_DATA_PATH = \"data/semcor_meta/meta_test_\" + str(SHOT) + \"-\" + str(SHOT)\n",
    "  CENTROIDS_OUTPUT_PATH = \"data/centroids/\"\n",
    "  TRAINING_ENCODINGS_PATH = \"data/encodings/\"\n",
    "  TEST_ENCODINGS_PATH = \"data/test_encodings/\"\n",
    "\n",
    "  centroid_sizes = {}\n",
    "  training_sets = []\n",
    "  total_files = 0\n",
    "  for file_name in os.listdir(TRAINING_DATA_PATH):\n",
    "    total_files += 1\n",
    "    print(\"Processing file\", total_files, \"with name\", file_name)\n",
    "    data = json.load(open(TRAINING_DATA_PATH + \"/\" + file_name))\n",
    "    for index in range(len(data)):\n",
    "      labeled_sentence = [data[index]['sentence'], data[index]['label']]\n",
    "      input_sentence = (' '.join(labeled_sentence[0]))    \n",
    "      required_index = UNDEF\n",
    "      for index_label in range(len(labeled_sentence[1])):\n",
    "        if labeled_sentence[1][index_label] != -1:\n",
    "          required_index = index_label\n",
    "          break\n",
    "      required_word = labeled_sentence[0][required_index]\n",
    "      inputs = tokenizer(required_word, return_tensors=\"pt\")\n",
    "      required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "      inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "      outputs = model(**inputs)\n",
    "      inputs_indices_sentence = []\n",
    "      for i in range(len(inputs['input_ids'].tolist()[0])):\n",
    "        if inputs['input_ids'].tolist()[0][i] in required_bert_tokens:\n",
    "          required_bert_tokens.remove(inputs['input_ids'].tolist()[0][i])\n",
    "          inputs_indices_sentence.append(i)        \n",
    "      encodings = []\n",
    "      for inputs_index_sentence in inputs_indices_sentence:\n",
    "        encodings.append(outputs.last_hidden_state[:,inputs_index_sentence,:])\n",
    "      encoding = torch.stack(encodings).mean(dim=0)\n",
    "      label = labeled_sentence[1][required_index]    \n",
    "\n",
    "      if label + \"_size\" in centroid_sizes:\n",
    "        with open(TRAINING_ENCODINGS_PATH + label, 'rb+') as f_enc:\n",
    "          data_points = np.load(f_enc)\n",
    "        if (encoding.detach().numpy() == data_points).any() != True:\n",
    "          data_points_updated = np.concatenate((data_points, encoding.detach().numpy()), axis=0)\n",
    "          with open(TRAINING_ENCODINGS_PATH + label, 'wb+') as f_enc:\n",
    "            np.save(f_enc, data_points_updated)\n",
    "          centroid_sizes[label + '_size'] += 1\n",
    "          with open(CENTROIDS_OUTPUT_PATH + label, 'rb+') as f:\n",
    "            old_average = np.load(f)\n",
    "          new_average = old_average + (encoding.detach().numpy() - old_average)/centroid_sizes[label + \"_size\"]\n",
    "          with open(CENTROIDS_OUTPUT_PATH + label, 'wb+') as f:\n",
    "            np.save(f, new_average)\n",
    "      else:\n",
    "        centroid_sizes[label + '_size'] = 1\n",
    "        with open(CENTROIDS_OUTPUT_PATH + label, 'wb+') as f:\n",
    "          np.save(f, encoding.detach().numpy())\n",
    "        with open(TRAINING_ENCODINGS_PATH + label, 'wb+') as f:\n",
    "          np.save(f, encoding.detach().numpy())      \n",
    "\n",
    "  test_data_sizes = {}\n",
    "  total_files = 0\n",
    "  for file_name in os.listdir(TEST_DATA_PATH):\n",
    "    total_files += 1\n",
    "    print(\"Processing file\", total_files)\n",
    "    data = json.load(open(TEST_DATA_PATH + \"/\" + file_name))\n",
    "    for index in range(len(data)):\n",
    "      labeled_sentence = [data[index]['sentence'], data[index]['label']]\n",
    "      input_sentence = (' '.join(labeled_sentence[0]))\n",
    "      required_index = UNDEF\n",
    "      for index_label in range(len(labeled_sentence[1])):\n",
    "        if labeled_sentence[1][index_label] != -1:\n",
    "          required_index = index_label\n",
    "          break\n",
    "      required_word = labeled_sentence[0][required_index]\n",
    "      inputs = tokenizer(required_word, return_tensors=\"pt\")\n",
    "      required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "      inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "      outputs = model(**inputs)\n",
    "      inputs_indices_sentence = []\n",
    "      for i in range(len(inputs['input_ids'].tolist()[0])):\n",
    "        if inputs['input_ids'].tolist()[0][i] in required_bert_tokens:\n",
    "          required_bert_tokens.remove(inputs['input_ids'].tolist()[0][i])\n",
    "          inputs_indices_sentence.append(i)        \n",
    "      encodings = []\n",
    "      for inputs_index_sentence in inputs_indices_sentence:\n",
    "        encodings.append(outputs.last_hidden_state[:,inputs_index_sentence,:])\n",
    "      encoding = torch.stack(encodings).mean(dim=0)\n",
    "      label = labeled_sentence[1][required_index]\n",
    "\n",
    "      if label + \"_size\" in test_data_sizes:\n",
    "        test_data_sizes[label + '_size'] += 1\n",
    "        with open(TEST_ENCODINGS_PATH + label, 'rb+') as f_enc:\n",
    "          data_points = np.load(f_enc)\n",
    "        if (encoding.detach().numpy() == data_points).any() != True:\n",
    "          data_points_updated = np.concatenate((data_points, encoding.detach().numpy()), axis=0)\n",
    "          with open(TEST_ENCODINGS_PATH + label, 'wb+') as f_enc:\n",
    "            np.save(f_enc, data_points_updated)        \n",
    "      else:\n",
    "        test_data_sizes[label + '_size'] = 1\n",
    "        with open(TEST_ENCODINGS_PATH + label, 'wb+') as f:\n",
    "          np.save(f, encoding.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create-leopard-environment.sh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers\n",
    "! git clone https://github.com/iesl/leopard.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create-lo-shot-environment.sh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required dependencies\n",
    "! pip install mathutils\n",
    "! pip install poisson-disc\n",
    "! pip install transformers\n",
    "! pip install mosek\n",
    "! pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**install-mosek.sh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp mosektoolslinux64x86.tar.bz2 $HOME/\n",
    "! cp mosek.lic $HOME/\n",
    "! cd $HOME/\n",
    "! tar -xf mosektoolslinux64x86.tar.bz2\n",
    "! mv $HOME/mosek.lic $HOME/mosek/\n",
    "! rm $HOME/mosektoolslinux64x86.tar.bz2\n",
    "! export PATH=/root/mosek/9.3/tools/platform/linux64x86/bin:$PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

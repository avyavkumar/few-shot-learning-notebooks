{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtP3I8wtFuj9"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiRKVWWY6WEy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "DATA_PATH = \"leopard/data/json\"\n",
    "\n",
    "def get_categories():\n",
    "    categories = []\n",
    "    categories.append(\"restaurant\")\n",
    "    categories.append(\"conll\")\n",
    "    return categories\n",
    "\n",
    "def get_labelled_training_sentences(category, shot, episode):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    entities = []\n",
    "    label_keys = {}\n",
    "    label_index = 0\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_\" + str(episode) + \"_\" + str(shot) + \".json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                sentence_1 = data[index]['sentence1']\n",
    "                entity = data[index]['sentence2']\n",
    "                label = data[index]['label']\n",
    "                sentences.append(sentence_1)\n",
    "                entities.append(entity)\n",
    "                # convert categorical labels to numeric values\n",
    "                if label not in label_keys:\n",
    "                    label_keys[label] = label_index\n",
    "                    label_index += 1\n",
    "                labels.append(label_keys[label])\n",
    "    return sentences, entities, labels, label_keys\n",
    "\n",
    "def get_labelled_test_sentences(category):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    entities = []\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_eval.json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                sentence_1 = data[index]['sentence1']\n",
    "                entity = data[index]['sentence2']\n",
    "                label = data[index]['label']\n",
    "                sentences.append(sentence_1)\n",
    "                labels.append(label)\n",
    "                entities.append(entity)\n",
    "    \n",
    "    return sentences, entities, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "        self.labels = labels\n",
    "        self.encodings = encodings\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.encodings)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.encodings[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0pBubF0OZfm"
   },
   "source": [
    "# **Extracting the data and creating the training set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG_TLr5xWQpc"
   },
   "source": [
    "Generate the sentence encoding for each sentence. Store this along with the labels; use it to generate lines and prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import traceback\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smNjhHgfWzg-"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "EXTRACT_FEATURES_COMMAND_BASE_COMPLETE = \"python mt-dnn/extractor_complete.py --do_lower_case --finput mt-dnn/input_examples/single-input.txt --foutput mt-dnn/input_examples/single-output.json --bert_model bert-base-uncased --checkpoint mt-dnn/mt_dnn_models/mt_dnn_base_uncased.pt\"\n",
    "EXTRACT_FEATURES_COMMAND_LARGE = \"python mt-dnn/extractor.py --do_lower_case --finput mt-dnn/input_examples/single-input.txt --foutput mt-dnn/input_examples/single-output.json --bert_model bert-base-uncased --checkpoint mt-dnn/mt_dnn_models/mt_dnn_large_uncased.pt\"\n",
    "\n",
    "def get_labelled_training_data(category, shot, episode):\n",
    "    sentences, entities, training_labels, label_keys = get_labelled_training_sentences(category, shot, episode)\n",
    "    # write all sentences to the input file\n",
    "    with open(\"mt-dnn/input_examples/single-input.txt\", 'w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(sentences))\n",
    "    # execute the command to get encodings\n",
    "    os.system(EXTRACT_FEATURES_COMMAND_BASE_COMPLETE)\n",
    "    # fetch sentence encodings from the output file\n",
    "    training_encodings = []\n",
    "    with open('mt-dnn/input_examples/single-output.json', 'r') as data_file:\n",
    "        encodings_json = data_file.read()\n",
    "    encodings_data = json.loads(encodings_json)\n",
    "    for i in range(len(sentences)):\n",
    "        inputs = tokenizer(entities[i], return_tensors=\"pt\")\n",
    "        required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "        outputs = np.array(ast.literal_eval(encodings_data[i]['11']), dtype=np.float32)\n",
    "        tokens = encodings_data[i]['token_ids']\n",
    "        encodings = []\n",
    "        for token_i in range(len(tokens)):\n",
    "            if tokens[token_i] in required_bert_tokens:\n",
    "                required_bert_tokens.remove(tokens[token_i])\n",
    "                encodings.append(outputs[token_i])\n",
    "        encoding = np.stack(encodings).mean(axis=0).reshape(-1)\n",
    "        training_encodings.append(encoding)\n",
    "\n",
    "    return training_encodings, training_labels, label_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N54LggdSZU2l"
   },
   "source": [
    "Create the test dataset for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GP50vfRaZUN-"
   },
   "outputs": [],
   "source": [
    "def get_labelled_test_data(category):\n",
    "    sentences, entities, test_labels = get_labelled_test_sentences(category)\n",
    "    # write all sentences to the input file\n",
    "    with open(\"mt-dnn/input_examples/single-input.txt\", 'w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(sentences))\n",
    "    # execute the command to get encodings\n",
    "    os.system(EXTRACT_FEATURES_COMMAND_BASE_COMPLETE)\n",
    "    # fetch sentence encodings from the output file\n",
    "    test_encodings = []\n",
    "    with open('mt-dnn/input_examples/single-output.json', 'r') as data_file:\n",
    "        encodings_json = data_file.read()\n",
    "    encodings_data = json.loads(encodings_json)\n",
    "    for i in range(len(sentences)):\n",
    "        inputs = tokenizer(entities[i], return_tensors=\"pt\")\n",
    "        required_bert_tokens = [e for e in inputs['input_ids'].tolist()[0] if e not in [101, 102]]\n",
    "        outputs = np.array(ast.literal_eval(encodings_data[i]['11']), dtype=np.float32)\n",
    "        tokens = encodings_data[i]['token_ids']\n",
    "        encodings = []\n",
    "        for token_i in range(len(tokens)):\n",
    "            if tokens[token_i] in required_bert_tokens:\n",
    "                required_bert_tokens.remove(tokens[token_i])\n",
    "                encodings.append(outputs[token_i])\n",
    "        encoding = np.stack(encodings).mean(axis=0).reshape(-1)\n",
    "        test_encodings.append(encoding)\n",
    "    \n",
    "    return test_encodings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "INPUT_DIMS = 768\n",
    "\n",
    "def get_model(output_dims):\n",
    "  return nn.Linear(INPUT_DIMS, output_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, trainloader, epochs):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimiser = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      # get the inputs; data is a list of [inputs, labels]\n",
    "      inputs, labels = data\n",
    "      # zero the parameter gradients\n",
    "      optimiser.zero_grad()\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimiser.step()\n",
    "      # print statistics\n",
    "      running_loss += loss.item()\n",
    "      # print(\"The loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "      encodings, labels = data\n",
    "      # calculate outputs by running images through the network\n",
    "      outputs = model(encodings)\n",
    "      # the class with the highest energy is what we choose as prediction\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "  return correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the training set\n",
    "# get a model from it and train the model\n",
    "# check the model performance on the test set\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "epochs = {4: 100,\n",
    "          8: 125,\n",
    "          16: 150}\n",
    "\n",
    "display_stats = False\n",
    "\n",
    "for category in get_categories():\n",
    "  test_encodings, categorical_test_labels = get_labelled_test_data(category)\n",
    "  accuracies = {}\n",
    "  for shot in [4,8,16]:\n",
    "    accuracies[shot] = []\n",
    "  for episode in range(3):\n",
    "    for shot in [4,8,16]:\n",
    "      predictions = []\n",
    "      true_labels = []\n",
    "\n",
    "      training_encodings, training_labels, label_keys = get_labelled_training_data(category, shot, episode)            \n",
    "      if not training_encodings:\n",
    "        continue\n",
    "    \n",
    "      # convert categorical attributes to numeric indices\n",
    "      test_labels = [label_keys[label] for label in categorical_test_labels]\n",
    "      \n",
    "      # define the model\n",
    "      classes = max(max(training_labels) for training_label in training_labels) + 1\n",
    "      model = get_model(output_dims=classes)\n",
    "            \n",
    "      # create the dataloaders for training and test splits\n",
    "      training_set = Dataset(training_encodings, training_labels)\n",
    "      train_loader = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "      test_set = Dataset(test_encodings, test_labels)\n",
    "      test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
    "      \n",
    "      # train the model\n",
    "      train(model, train_loader, epochs=epochs[shot])\n",
    "      \n",
    "      # test the model performance\n",
    "      correct, total = test(model, test_loader)\n",
    "      \n",
    "      if display_stats == True:\n",
    "        print(\"The training split is\", len(training_encodings))\n",
    "        print(\"The test split is\", len(test_encodings))\n",
    "        print(\"The number of classes are\", classes)\n",
    "        print(\"For category\", category, \"and shot =\", str(shot) + \"...\")\n",
    "        print(\"Accuracy is\", correct/total, \"\\n\")\n",
    "      accuracies[shot].append(correct/total)\n",
    "  print(\"\\n\" + \"For category \" + category + \"...\")\n",
    "  for shot in [4,8,16]:\n",
    "    print(\"The accuracy is\", round(np.mean(accuracies[shot]), 4), \"+-\", round(np.std(accuracies[shot]), 4), \"for shot =\", shot)\n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "entity-typing-mt-dnn.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

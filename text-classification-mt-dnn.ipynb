{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5PMR-533jsJ"
   },
   "source": [
    "# **Generate data and install Mosek**\n",
    "\n",
    "Please install Mosek according to the instructions on [https://docs.mosek.com/latest/install/installation.html](https://docs.mosek.com/latest/install/installation.html).\n",
    "\n",
    "Before running this cell, run the following in bash - \n",
    "\n",
    "`export PATH=/root/mosek/9.3/tools/platform/linux64x86/bin:$PATH`.\n",
    "\n",
    "Please download MOSEK from [here](https://drive.google.com/drive/folders/1ZIKizbByyQIZmWCPTgFvnnQ6bVzUcRvF?usp=sharing) and add all contents to the current working directory. \n",
    "\n",
    "Please replace *mosek.lic* with your own license file obtained from [here](https://www.mosek.com/products/academic-licenses/).\n",
    "\n",
    "Please download *MT-DNN.zip* which contains all models and scripts used in this notebook from [here](https://drive.google.com/file/d/1Wq4V93ZZC3sraVRY62agwXIJC0PubuGF/view?usp=sharing) and place it in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VtcTLLJIHHF"
   },
   "outputs": [],
   "source": [
    "! bash install-mosek.sh\n",
    "! unzip mt-dnn.zip\n",
    "! rm mt-dnn.zip\n",
    "! bash create-leopard-environment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtP3I8wtFuj9"
   },
   "source": [
    "# **LEOPARD**\n",
    "\n",
    "Setup the environment required for the classfications which are defined by Bansal et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiRKVWWY6WEy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATA_PATH = \"/content/leopard/data/json\"\n",
    "\n",
    "def get_categories():\n",
    "    categories = os.listdir(DATA_PATH)\n",
    "    categories.remove(\"restaurant\")\n",
    "    categories.remove(\"conll\")\n",
    "\n",
    "    # move emotion to the end\n",
    "    categories.remove(\"emotion\")\n",
    "    categories.append(\"emotion\")\n",
    "    return categories\n",
    "\n",
    "def get_labelled_training_sentences(category, shot, episode):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    label_keys = {}\n",
    "    label_index = 0\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_\" + str(episode) + \"_\" + str(shot) + \".json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                processed_sentence = data[index]['processed_sent']\n",
    "                processed_sentence = processed_sentence.replace('[CLS]', '')\n",
    "                processed_sentence = processed_sentence.replace('[SEP]', '')\n",
    "                processed_sentence = processed_sentence.replace('[PAD]', '')\n",
    "                label = data[index]['label']\n",
    "                sentences.append(processed_sentence)\n",
    "                # convert categorical labels to numeric values\n",
    "                if label not in label_keys:\n",
    "                    label_keys[label] = label_index\n",
    "                    label_index += 1\n",
    "                labels.append(label_keys[label])\n",
    "    return sentences, labels, label_keys\n",
    "\n",
    "def get_labelled_test_sentences(category):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    data_path = DATA_PATH + \"/\" + category + \"/\"\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith(\"_eval.json\"):\n",
    "            data = json.load(open(data_path + file_name))            \n",
    "            for index in range(len(data)):\n",
    "                processed_sentence = data[index]['processed_sent']\n",
    "                processed_sentence = processed_sentence.replace('[CLS]', '')\n",
    "                processed_sentence = processed_sentence.replace('[SEP]', '')\n",
    "                processed_sentence = processed_sentence.replace('[PAD]', '')\n",
    "                label = data[index]['label']\n",
    "                sentences.append(processed_sentence)\n",
    "                labels.append(label)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7nZf0I2VdeX"
   },
   "source": [
    "# **Set Up MT-DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsGNkwiBVy0C"
   },
   "outputs": [],
   "source": [
    "! pip install pytorch_pretrained_bert==0.4.0\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/content/mt-dnn/')\n",
    "\n",
    "EXTRACT_FEATURES_COMMAND_BASE = \"python mt-dnn/extractor.py --do_lower_case --finput mt-dnn/input_examples/single-input.txt --foutput mt-dnn/input_examples/single-output.json --bert_model bert-base-uncased --checkpoint mt-dnn/mt_dnn_models/mt_dnn_base_uncased.pt\"\n",
    "EXTRACT_FEATURES_COMMAND_LARGE = \"python mt-dnn/extractor.py --do_lower_case --finput mt-dnn/input_examples/single-input.txt --foutput mt-dnn/input_examples/single-output.json --bert_model bert-base-uncased --checkpoint mt-dnn/mt_dnn_models/mt_dnn_large_uncased.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIQZ72A0I82v"
   },
   "source": [
    "# **Less Than One Shot Classification (Sucholutsky et al, 2021)**\n",
    "\n",
    "Original source can be found [here](https://github.com/ilia10000/LO-Shot/blob/master/Paper3/Soft%20Label%20Optimization.ipynb). Run the script to install the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWa13LwHJYHU"
   },
   "outputs": [],
   "source": [
    "! bash create-lo-shot-environment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69r4I4qSLMk8"
   },
   "source": [
    "Load the required scripts and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3TnTswoyBxm"
   },
   "outputs": [],
   "source": [
    "% run lo_shot_definitions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0pBubF0OZfm"
   },
   "source": [
    "# **Extracting the data and creating the training set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG_TLr5xWQpc"
   },
   "source": [
    "Generate the sentence encoding for each sentence. Store this along with the labels; use it to generate lines and prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smNjhHgfWzg-"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_labelled_training_data(category, shot, episode):    \n",
    "    sentences, training_labels, label_keys = get_labelled_training_sentences(category, shot, episode)  \n",
    "    # write all sentences to the input file\n",
    "    with open(\"/content/mt-dnn/input_examples/single-input.txt\", 'w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(sentences))\n",
    "    # execute the command to get encodings\n",
    "    os.system(EXTRACT_FEATURES_COMMAND_BASE)\n",
    "    # fetch sentence encodings from the output file\n",
    "    training_encodings = []\n",
    "    with open('/content/mt-dnn/input_examples/single-output.json', 'r') as data_file:\n",
    "        encodings_json = data_file.read()\n",
    "    encodings_data = json.loads(encodings_json)\n",
    "    for encoding in encodings_data:\n",
    "        training_encodings.append(np.array(ast.literal_eval(encoding['11'])))        \n",
    "    return training_encodings, training_labels, label_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL43mUWtfHcE"
   },
   "source": [
    "Get centroids for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckbTb8m5fJV7"
   },
   "outputs": [],
   "source": [
    "def get_labelled_centroids(training_encodings, training_labels):\n",
    "    centroids = []\n",
    "    centroid_labels = []\n",
    "    for label in set(training_labels):\n",
    "        centroids_per_label = []\n",
    "        for i in range(len(training_labels)):\n",
    "            if training_labels[i] == label:\n",
    "                centroids_per_label.append(training_encodings[i])\n",
    "        centroid = np.mean(np.array(centroids_per_label), axis=0)\n",
    "        centroids.append(centroid)\n",
    "        centroid_labels.append(label)\n",
    "    \n",
    "    return centroids, centroid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N54LggdSZU2l"
   },
   "source": [
    "Create the test dataset for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GP50vfRaZUN-"
   },
   "outputs": [],
   "source": [
    "def get_labelled_test_data(category):\n",
    "    sentences, test_labels = get_labelled_test_sentences(category)\n",
    "    # write all sentences to the input file\n",
    "    with open(\"/content/mt-dnn/input_examples/single-input.txt\", 'w', encoding='utf-8') as writer:\n",
    "        writer.write('\\n'.join(sentences))\n",
    "    # execute the command to get encodings\n",
    "    os.system(EXTRACT_FEATURES_COMMAND_BASE)\n",
    "    # fetch sentence encodings from the output file\n",
    "    test_encodings = []\n",
    "    with open('/content/mt-dnn/input_examples/single-output.json', 'r') as data_file:\n",
    "        encodings_json = data_file.read()\n",
    "    encodings_data = json.loads(encodings_json)\n",
    "    for encoding in encodings_data:        \n",
    "        test_encodings.append(np.array(ast.literal_eval(encoding['11'])))        \n",
    "    return test_encodings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVzjqFeiET3p"
   },
   "source": [
    "# **Classification using different flavours of KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFTreANOHINg"
   },
   "source": [
    "Perform the classification using soft label KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J47648URHLSJ"
   },
   "outputs": [],
   "source": [
    "def get_soft_label_prototypes(lines, labeled_centroids):\n",
    "\n",
    "  classifiers = []\n",
    "  for line in lines:\n",
    "    try:\n",
    "      distX, distY = get_line_prototypes(line, labeled_centroids[0])\n",
    "      classifier = SoftKNN(k=1)\n",
    "      classifier.fit(distX, distY)\n",
    "      classifiers.append(classifier)\n",
    "      print(\"distX is\", distX)\n",
    "      print(\"distY is\", distY)\n",
    "    except:\n",
    "      classifiers.append(None)\n",
    "\n",
    "  return classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtVVUrzx-aDw"
   },
   "source": [
    "# **Classify and measure metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Kwq29eFvFj"
   },
   "source": [
    "Classify the test data using centroids from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeHDtBDbMb8F"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert(cp.MOSEK in cp.installed_solvers())\n",
    "\n",
    "for category in get_categories():\n",
    "    test_encodings, categorical_test_labels = get_labelled_test_data(category)\n",
    "    for episode in range(10):\n",
    "        for shot in [4,8,16]:\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "\n",
    "            training_encodings, training_labels, label_keys = get_labelled_training_data(category, shot, episode)            \n",
    "            if not training_encodings:\n",
    "                continue\n",
    "            # convert categorical attributes to numeric indices\n",
    "            test_labels = [label_keys[label] for label in categorical_test_labels]        \n",
    "            centroids, centroid_labels = get_labelled_centroids(training_encodings, training_labels)\n",
    "\n",
    "            labeled_training_data_np = np.stack(training_encodings, axis=0)\n",
    "            labeled_test_data_np = np.stack(test_encodings, axis=0)\n",
    "            labeled_centroids_np = np.stack(centroids, axis=0)\n",
    "\n",
    "            labeled_test_data = [labeled_test_data_np[i] for i in range(labeled_test_data_np.shape[0])], [label for label in test_labels]\n",
    "            labeled_centroids = [labeled_centroids_np[i] for i in range(labeled_centroids_np.shape[0])], [label for label in centroid_labels]\n",
    "\n",
    "            test_classifications = max(max(labeled_centroids[1]) for labeled_centroid in labeled_centroids) + 1\n",
    "            required_lines = test_classifications - 1\n",
    "            dimensions = labeled_training_data_np.shape[1]\n",
    "\n",
    "            lines = [line_order_no_endpoints(centroids=labeled_centroids_np, active_classes=np.array(line)) for line in find_lines_R_multiD(dat=labeled_training_data_np, labels=training_labels , dims=dimensions, centroids=labeled_centroids_np, k=required_lines)]\n",
    "            classifiers = get_soft_label_prototypes(lines, labeled_centroids)\n",
    "\n",
    "            for i in range(test_classifications):\n",
    "                points = int(np.sum([True for class_val in labeled_test_data[1] if class_val == i]))\n",
    "                points_required = [labeled_test_data[0][x] for x in range(len(labeled_test_data[1])) if labeled_test_data[1][x] == i]\n",
    "                assignments = []\n",
    "                for point in points_required:\n",
    "                    dists = [dist_to_line_multiD(point, labeled_centroids[0][line[0]], labeled_centroids[0][line[-1]]) for line in lines]\n",
    "                    nearest = np.argmin(dists)\n",
    "                    assignments.append(nearest)\n",
    "                for j in range(len(points_required)):\n",
    "                    if classifiers[assignments[j]] is not None:\n",
    "                        classifier = classifiers[assignments[j]]\n",
    "                        prediction = classifier.predict(points_required[j])\n",
    "                        predictions.append(prediction)\n",
    "                        true_labels.append(i)\n",
    "\n",
    "            print(\"For category\", category, \"and shot =\", str(shot) + \"...\")\n",
    "            print(\"Lines used are\", len(lines))\n",
    "            print(\"Number of classifications are\", test_classifications)\n",
    "            print(\"Macro f1 score is\", f1_score(true_labels, predictions, average='macro'))\n",
    "            print(\"Accuracy is\", accuracy_score(true_labels, predictions))\n",
    "            print(\"Correctly classified points are\", np.sum(np.array(true_labels) == np.array(predictions)), \"/\", len(true_labels), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "class-episodic-mt-dnn.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
